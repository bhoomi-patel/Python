'''Custom NER means teaching an NLP model to find and label specific "entities" (names, products, IDs, custom medical/drug names, etc.) in text, where the set of entities or their types is specialized for your application.'''
'''Why and When?
General NER models ("PER", "ORG", "LOC") are powerful, but you may need something unique:
Medical NER: Drug names, diseases, gene mutations, symptoms.
Brand/Product NER: Brand names, model numbers.
Legal/Contract NER: Clause names, dates, case/precedent names.
Your own companyâ€™s jargon or confidential identifiers.'''
'''Key Concepts
Annotation: Marking up text with what words/phrases belong to which entity classes.
Entity Types: You define them! E.g., "PRODUCT", "BRAND", "INGREDIENT".
Fine-tuning: Adapting a pre-trained model to spot your entity types using labeled training examples.
Evaluation: Test on a held-out set and check Precision/Recall/F1 for each type.'''

# NER Data Annotation (BIO Scheme)
''' To train a custom NER model, you need labeled data. The BIO (Beginning, Inside, Outside) scheme is the most common way to annotate text for NER tasks. It assigns a tag to each word (or subword token) indicating if it's the beginning of an entity, inside an entity, or outside of any entity.'''
'''Key Concepts:
B-TYPE: B- denotes the Beginning of an entity of a specific TYPE (e.g., B-PERSON, B-LOCATION).
I-TYPE: I- denotes an Inside token of an entity of a specific TYPE. It must follow a B-TYPE or another I-TYPE of the same type.
O: O denotes an Outside token, meaning it's not part of any named entity.
Token-level Labels: Each word in the original sentence gets one BIO tag.'''
# raw sentence 
sentence = "Barack Obama visited Google in Mountain View."
# Corresponding words (tokenized)
words = ["Barack", "Obama", "visited", "Google", "in", "Mountain", "View", "."]
# Manually annotated BIO labels
# B-PER: Beginning of a Person entity
# I-PER: Inside a Person entity
# B-ORG: Beginning of an Organization entity
# I-LOC: Inside a Location entity
# O: Outside of any entity
bio_labels = [
    "B-PERSON", "I-PERSON", "O",
    "B-ORG", "O",
    "B-LOCATION", "I-LOCATION", "O"
]
print("\nOriginal Sentence:")
print(sentence)
# Example of a slightly more complex sentence
sentence_2 = "Apple Inc. announced a new iPhone 15 launch event in Cupertino, California on September 12th."
words_2 = ["Apple", "Inc.", "announced", "a", "new", "iPhone", "15", "launch", "event", "in", "Cupertino", ",", "California", "on", "September", "12th", "."]
bio_labels_2 = [
    "B-ORG", "I-ORG", "O", "O", "O",
    "B-PRODUCT", "I-PRODUCT", "O", "O", "O",
    "B-LOCATION", "O", "I-LOCATION", "O",
    "B-DATE", "I-DATE", "O"
]

print("\nAnother Example Annotation (with PRODUCT and DATE):")
for word, label in zip(words_2, bio_labels_2):
    print(f"  '{word}': {label}")
# The order matters as it defines the integer ID (e.g., O=0, B-PERSON=1, I-PERSON=2, ...)
unique_labels = sorted(list(set([tag.split('-')[-1] if tag != 'O' else 'O' for tag in bio_labels + bio_labels_2])))
# Now create the actual B- and I- tags
ner_labels = ['O'] + [f'{prefix}-{label_type}' for label_type in sorted(list(set(unique_labels) - {'O'})) for prefix in ['B', 'I']]
print(f"\nUnique NER Labels (ordered for model mapping): {ner_labels}")
label_to_id = {label: i for i, label in enumerate(ner_labels)}
id_to_label = {i: label for i, label in enumerate(ner_labels)}
print(f"Label to ID mapping: {label_to_id}")

#  Preparing Data for Transformer NER Fine-tuning
'''Transformer models use subword tokenizers (e.g., WordPiece, SentencePiece) that often split a single word into multiple subword tokens (e.g., "programming" -> "program", "##ming"). When applying BIO labels, which are typically at the word level, we need to align these word-level labels with the subword tokens. This process ensures each subword token gets an appropriate label.'''
'''Key Concepts:

Subword Tokenization: Transformers break words into smaller units to handle OOV words and reduce vocabulary size.
Label Alignment: The crucial step of mapping word-level BIO tags to the subword tokens generated by the tokenizer.
tokenizer.word_ids(): A Hugging Face tokenizer method that maps each subword token ID back to its original word ID. This is essential for alignment.
'-100' for Special Tokens: Special tokens ([CLS], [SEP], padding) and subsequent subword tokens (after the first subword of an original word) usually get a label of -100 to indicate they should be ignored by the loss function.'''
from transformers import AutoTokenizer
import torch
from torch.utils.data import Dataset

# 1. Initialize tokenizer
model_checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# 2. sample data (words and corresponding BIO labels)
words = ["Barack", "Obama", "visited", "Google", "in", "Mountain", "View", "."]
word_labels = ["B-PERSON", "I-PERSON", "O", "B-ORG", "O", "B-LOCATION", "I-LOCATION", "O"]

# Define the mapping from string labels to integer IDs
# (Must be consistent with `id_to_label` later for evaluation)
tag_names = ["O", "B-PERSON", "I-PERSON", "B-ORG", "I-ORG", "B-LOCATION", "I-LOCATION", "B-PRODUCT", "I-PRODUCT", "B-DATE", "I-DATE"]
label_to_id = {tag: i for i, tag in enumerate(tag_names)}
id_to_label = {i: tag for i, tag in enumerate(tag_names)}

print(f"\nLabel to ID mapping: {label_to_id}")
# 3. Tokenize Words and Align Labels
# This function handles the complex alignment
def tokenize_and_align_labels(words, word_labels, tokenizer, label_to_id):
    tokenized_inputs = tokenizer(words, is_split_into_words=True, truncation=True, padding="max_length", max_length=128)
    labels = []
    # word_ids() maps each token back to the index of the word it came from in the original list
    # Example: ['Barack', '## Obama'] -> [0, 1]
    #           ^token_ids           ^word_ids
    for word_idx in tokenized_inputs.word_ids():
        if word_idx is None: # Special tokens ([CLS], [SEP], padding)
            labels.append(-100) # -100 tells PyTorch to ignore this token for loss calculation
        else:
            # Assign the label of the original word to its first subword token
            # For subsequent subwords of the same original word, use -100
            # This is a common strategy to avoid over-counting a word's label
            if word_idx != previous_word_idx: # if it's the first subword of a new original word
                labels.append(label_to_id[word_labels[word_idx]])
            else:
                labels.append(-100) # Subsequent subwords get -100

        previous_word_idx = word_idx

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Example usage:
encoded_sample = tokenize_and_align_labels(words, word_labels, tokenizer, label_to_id)

print(f"\nOriginal words: {words}")
print(f"Original labels: {word_labels}")
print(f"Tokenized IDs: {encoded_sample['input_ids']}")
print(f"Tokenized Tokens: {[tokenizer.decode(t) for t in encoded_sample['input_ids']]}")
print(f"Word IDs (map token back to original word index): {encoded_sample.word_ids()}")
print(f"Aligned Labels: {encoded_sample['labels']}")

# Let's decode the aligned labels for clarity
decoded_aligned_labels = [id_to_label[label_id] if label_id != -100 else "IGNORE" for label_id in encoded_sample['labels']]
print(f"Decoded Aligned Labels: {decoded_aligned_labels}")

# --- Custom Dataset Class for Hugging Face Trainer ---
class NERDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        # Convert lists of integers to PyTorch tensors
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings['input_ids'])

# Example: Create dummy dataset
mock_data = {
    'input_ids': [encoded_sample['input_ids']],
    'token_type_ids': [encoded_sample['token_type_ids']],
    'attention_mask': [encoded_sample['attention_mask']],
    'labels': [encoded_sample['labels']]
}
mock_dataset = NERDataset(mock_data)
print(f"\nMock NER Dataset created. Length: {len(mock_dataset)}")

# Building and Fine-tuning a Custom NER Model (PyTorch & Hugging Face Trainer)
'''We'll use a pre-trained Transformer model (like BERT) that has a classification head specifically designed for token-level classification (which is what NER is). We then fine-tune this model on our custom, labeled dataset using the Hugging Face Trainer API.'''
'''Key Concepts:

AutoModelForTokenClassification: Hugging Face class to load a pre-trained Transformer with a sequence tagging head on top.
num_labels: Must be set to the number of unique BIO tags you have.
Metrics: For NER, standard metrics are Precision, Recall, F1-score, often calculated per entity type and averaged, taking into account correct span predictions. seqeval is a common library for this.'''
from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer
import torch
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
import numpy as np
from seqeval.metrics import classification_report, f1_score, precision_score, recall_score # type: ignore # For NER-specific metrics
# --- 1. Define Model Checkpoint, Labels, and Mappings ---
model_checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Re-define consistent label mappings
# Using a small custom set of entity types for simplicity
custom_tags = ["O", "B-PERSON", "I-PERSON", "B-ORG", "I-ORG", "B-LOCATION", "I-LOCATION"]
id_to_label = {i: tag for i, tag in enumerate(custom_tags)}
label_to_id = {tag: i for i, tag in enumerate(custom_tags)}
num_labels = len(custom_tags)

print(f"\nCustom NER Labels: {custom_tags}")
print(f"Number of custom labels: {num_labels}")

# --- 2. Synthetic Dataset (for demo purposes) ---
# In real life, this would be a loaded and preprocessed dataset.
# We create small sentences with custom entities.
raw_train_texts = [
    ["My", "name", "is", "Alice", "Smith", "."],
    ["I", "work", "at", "Google", "Inc", "in", "Mountain", "View", "."],
    ["John", "Doe", "travels", "to", "Paris", "often", "."],
    ["Microsoft", "Corp", "is", "based", "in", "Redmond", "."],
    ["He", "met", "Dr.", "Jane", "Doe", "at", "the", "conference", "."]
]
raw_train_labels = [
    ["O", "O", "O", "B-PERSON", "I-PERSON", "O"],
    ["O", "O", "O", "B-ORG", "I-ORG", "O", "B-LOCATION", "I-LOCATION", "O"],
    ["B-PERSON", "I-PERSON", "O", "O", "B-LOCATION", "O", "O"],
    ["B-ORG", "I-ORG", "O", "O", "O", "B-LOCATION", "O"],
    ["O", "O", "O", "B-PERSON", "I-PERSON", "O", "O", "O", "O"]
]

# Split into train and validation sets
X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(
    raw_train_texts, raw_train_labels, test_size=0.2, random_state=42
)

# 3. Tokenize and Align Labels for all data
def tokenize_and_align_labels(words_list, word_labels_list, tokenizer, label_to_id):
    # Process each sentence individually
    all_tokenized_inputs = {'input_ids': [], 'attention_mask': [], 'labels': []}

    for words, word_labels in zip(words_list, word_labels_list):
        tokenized_inputs = tokenizer(words, is_split_into_words=True, truncation=True, padding="max_length", max_length=128)
        labels = []
        previous_word_idx = None

        for word_idx in tokenized_inputs.word_ids():
            if word_idx is None:
                labels.append(-100)
            elif word_idx != previous_word_idx:
                labels.append(label_to_id[word_labels[word_idx]])
            else:
                labels.append(-100) # Subsequent subwords get -100
            previous_word_idx = word_idx

        all_tokenized_inputs['input_ids'].append(tokenized_inputs['input_ids'])
        all_tokenized_inputs['attention_mask'].append(tokenized_inputs['attention_mask'])
        all_tokenized_inputs['labels'].append(labels)
    return all_tokenized_inputs

train_encodings = tokenize_and_align_labels(X_train_raw, y_train_raw, tokenizer, label_to_id)
val_encodings = tokenize_and_align_labels(X_val_raw, y_val_raw, tokenizer, label_to_id)

# 4. Custom Dataset Class (from 9.1.2)
class NERDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    def __getitem__(self, idx):
        # Ensure proper tensor conversion for each item
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
    def __len__(self):
        return len(self.encodings['input_ids'])

train_dataset = NERDataset(train_encodings)
val_dataset = NERDataset(val_encodings)

# 5. Load Model with Token Classification Head
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=num_labels, id2label=id_to_label, label2id=label_to_id)

# 6. Define Compute Metrics Function for NER
def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2) # Convert logits to class IDs

    # Remove ignored index (labels == -100)
    true_labels = [[id_to_label[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [id_to_label[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    # Use seqeval metrics
    return {
        "precision": precision_score(true_labels, true_predictions),
        "recall": recall_score(true_labels, true_predictions),
        "f1": f1_score(true_labels, true_predictions),
        "report": classification_report(true_labels, true_predictions) # Optional: full report
    }

# 7. Training Arguments
training_args = TrainingArguments(
    output_dir='./ner_results',
    num_train_epochs=5,                  # More epochs for better learning on small data
    per_device_train_batch_size=2,       # Small batch for tiny dataset
    per_device_eval_batch_size=2,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model='f1',
    logging_dir='./ner_logs',
    logging_steps=10,
    report_to="none"
)

# 8. Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer # Pass tokenizer to trainer for internal use (e.g., logging)
)

# 9. Train the Model
print("\n--- Starting Custom NER Fine-tuning ---")
trainer.train()
print("\nCustom NER Fine-tuning complete.")

# 10. Evaluate Final Model
print("\n--- Evaluating Fine-tuned Custom NER Model ---")
eval_results = trainer.evaluate()
print(f"Evaluation Results: {eval_results}")
# Also print the full classification report from the last eval
# Access the report via eval_results['eval_report'] if compute_metrics returns it
# print(eval_results.get('eval_report', 'Full report not available in eval_results directly.'))



# Inference with the Custom NER Model
'''After training, you need to use your custom NER model to predict entities in new, unseen text. This involves loading the saved model and tokenizer, processing new text, running inference, and then mapping the predicted token IDs back to human-readable entity spans.'''

import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification
import os
# --- 1. Load the Fine-tuned Model and Tokenizer ---
model_save_path = './ner_results/checkpoint-4' # Or the final model from trainer.save_model()
# Ensure this path exists after you run the training
if not os.path.exists(model_save_path):
    print(f"Model checkpoint path not found: {model_save_path}. Please run the fine-tuning first.")
    # As a fallback for demonstration, load the base model, but it won't have custom entities
    # loaded_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    # loaded_model = AutoModelForTokenClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)
    exit() # Exit if model is not found

loaded_tokenizer = AutoTokenizer.from_pretrained(model_save_path)
loaded_model = AutoModelForTokenClassification.from_pretrained(model_save_path)

# Ensure model is on the correct device and in evaluation mode
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
loaded_model.to(device)
loaded_model.eval()

# Re-define id_to_label (must match what was used during training)
id_to_label = loaded_model.config.id2label # Models save their id2label mapping!
print(f"\nLoaded model labels mapping: {id_to_label}")

# --- 2. Process New Text for Inference ---
inference_text = "Alice Smith works at Acme Corp in New York City."
# Tokenize the raw string for inference
inputs = loaded_tokenizer(inference_text, return_tensors="pt", truncation=True, padding=True)

# --- 3. Run Inference ---
with torch.no_grad():
    outputs = loaded_model(**{k: v.to(device) for k, v in inputs.items()})
logits = outputs.logits # Raw scores for each label at each token position

# Get predicted label IDs for each token
predicted_token_ids = torch.argmax(logits, dim=2)[0].cpu().numpy() # [0] to get first sentence in batch

# --- 4. Post-process Predictions to Extract Entities ---
words_in_text = loaded_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
word_ids = inputs.word_ids(batch_index=0) # Map tokens back to original words (useful for reconstructing entities)

current_entity_type = None
current_entity_start = -1
extracted_entities = []

# Iterate through tokens and their predicted labels
for i, (token_id, pred_id) in enumerate(zip(inputs['input_ids'][0], predicted_token_ids)):
    token = loaded_tokenizer.decode(token_id)
    label = id_to_label[pred_id]
    original_word_idx = word_ids[i]

    # Skip special tokens and padding
    if original_word_idx is None or label == "O":
        if current_entity_type is not None: # End of an entity
            # Reconstruct the original word from subword tokens
            entity_tokens = [loaded_tokenizer.decode(t) for t in inputs['input_ids'][0][current_entity_start:i] if t not in loaded_tokenizer.all_special_ids]
            extracted_entities.append({
                'entity': loaded_tokenizer.decode(inputs['input_ids'][0][current_entity_start:i]).replace(' ##', '').strip(), # Simple heuristic for subwords
                'type': current_entity_type
            })
            current_entity_type = None
        continue

    label_prefix, label_type = label.split('-', 1) if '-' in label else (label, None)

    if label_prefix == 'B':
        if current_entity_type is not None: # End previous entity if any
             entity_tokens = [loaded_tokenizer.decode(t) for t in inputs['input_ids'][0][current_entity_start:i] if t not in loaded_tokenizer.all_special_ids]
             extracted_entities.append({
                'entity': loaded_tokenizer.decode(inputs['input_ids'][0][current_entity_start:i]).replace(' ##', '').strip(),
                'type': current_entity_type
             })
        current_entity_type = label_type
        current_entity_start = i
    elif label_prefix == 'I':
        if current_entity_type is None or label_type != current_entity_type:
            # Malformed sequence (I-tag without B-tag, or type mismatch) - treat as new entity or O
            if current_entity_type is not None:
                 entity_tokens = [loaded_tokenizer.decode(t) for t in inputs['input_ids'][0][current_entity_start:i] if t not in loaded_tokenizer.all_special_ids]
                 extracted_entities.append({
                    'entity': loaded_tokenizer.decode(inputs['input_ids'][0][current_entity_start:i]).replace(' ##', '').strip(),
                    'type': current_entity_type
                 })
            current_entity_type = label_type
            current_entity_start = i
        # else: continue with current entity

# Check for entity at the very end of the sequence
if current_entity_type is not None:
    entity_tokens = [loaded_tokenizer.decode(t) for t in inputs['input_ids'][0][current_entity_start:len(inputs['input_ids'][0])] if t not in loaded_tokenizer.all_special_ids]
    extracted_entities.append({
        'entity': loaded_tokenizer.decode(inputs['input_ids'][0][current_entity_start:len(inputs['input_ids'][0])]).replace(' ##', '').strip(),
        'type': current_entity_type
    })


print(f"\nInference Text: '{inference_text}'")
print("\nExtracted Entities:")
for entity in extracted_entities:
    print(f"  - Entity: '{entity['entity']}', Type: {entity['type']}")
